# Risks & Misuses

import { Callout } from 'nextra-theme-docs'
import {Cards, Card} from 'nextra-theme-docs'
import {FilesIcon} from 'components/icons'

Well-crafted prompts can lead to effective used of LLMs for various tasks using techniques like few-shot learning and chain-of-thought prompting. As you think about building real-world applications on top of LLMs, it also becomes crucial to think about the misuses, risks, and safety practices involved with language models. 

This section focuses on highlighting some of the risks and misuses of LLMs via techniques like prompt injections. It also highlights harmful behaviors and how to potentially mitigate them via effective prompting techniques and tools like moderation APIs. Other topics of interest include generalizability, calibration, biases, social biases, and factuality to name a few.

<Cards>
  <Card
    icon={<FilesIcon />}
    title="Adversarial Prompting"
    href="/risks/adversarial"
  />
  <Card
    icon={<FilesIcon />}
    title="Factuality"
    href="/risks/factuality"
  />
  <Card
    icon={<FilesIcon />}
    title="Biases"
    href="/risks/biases"
  />
</Cards>

